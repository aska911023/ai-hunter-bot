import time
import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

# æ¨¡æ“¬ç€è¦½å™¨ Headerï¼Œé¿å…è¢«æ“‹
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def search_web(query, max_results=5):
    """
    å°æ‡‰åœ–ç‰‡ä¸­çš„ [SERP] å€å¡Šï¼šå»æœå°‹å¼•æ“æ‰¾è³‡æ–™
    """
    results = []
    print(f"ğŸ•µï¸â€â™‚ï¸ Searching for: {query}...")
    
    with DDGS() as ddgs:
        # ä½¿ç”¨ DuckDuckGo æœå°‹
        ddgs_gen = ddgs.text(query, max_results=max_results)
        for r in ddgs_gen:
            results.append({
                "title": r['title'],
                "link": r['href'],
                "snippet": r['body']
            })
            
    return results

def crawl_website(url):
    """
    å°æ‡‰åœ–ç‰‡ä¸­çš„ [Crawler] å€å¡Šï¼šæ·±å…¥ç¶²é æŠ“å– HTML
    """
    try:
        # å°æ‡‰åœ–ç‰‡ä¸­çš„ [Rate Limit]ï¼šä¼‘æ¯ä¸€ä¸‹ï¼Œé¿å…å¤ªå¿«è¢«å°é–
        time.sleep(1.5) 
        
        response = requests.get(url, headers=HEADERS, timeout=5)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. æŠ“æ¨™é¡Œ (og:title > title)
        title = soup.find("meta", property="og:title")
        title = title["content"] if title else soup.title.string if soup.title else ""
        
        # 2. æŠ“æè¿° (og:description > description)
        desc = soup.find("meta", property="og:description")
        if not desc:
            desc = soup.find("meta", attrs={"name": "description"})
        summary = desc["content"] if desc else ""
        
        # 3. æŠ“åœ–ç‰‡ (og:image)
        image = soup.find("meta", property="og:image")
        image_url = image["content"] if image else ""

        return {
            "title": title.strip(),
            "summary": summary.strip()[:200], # é™åˆ¶é•·åº¦
            "image_url": image_url,
            "link": url
        }
    except Exception as e:
        print(f"âŒ Crawl failed for {url}: {e}")
        return None